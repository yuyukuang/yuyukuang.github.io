---
title: 机器学习笔记（三）：SVM核函数等 #标题
categories: 机器学习 #分类，可多个；若单个去掉括号
tags: [深度学习, 算法] #标签，可多个；若单个去掉括号
mathjax: true #是否引用公式
kewords: #本文关键词
description: #本文描述，若为空就自动全文前150字)
copyright: true #是否需要版权申明，默认有
reward: true #是否需要打赏，默认有
toc: true #是否需要目录，默认有
password: #是否加密，为空不加密
date: 2018-1-25 17:50:02 #日期
---


# SVM

## SVM核函数包括：
1. 线性核函数、
2. 多项式核函数、
3. 径向基核函数、
4. 高斯核函数、
5. 幂指数核函数、
6. 拉普拉斯核函数、
7. ANOVA核函数、
8. 二次有理核函数、
9. 多元二次核函数、
10. 逆多元二次核函数
11. Sigmoid核函数

支持向量机是建立在统计学习理论基础之上的新一代机器学习算法，支持向量机的优势主要体现在解决线性不可分问题，它通过引入核函数，巧妙地解决了在高维空间中的内积运算，从而很好地解决了非线性分类问题。

构造出一个具有良好性能的SVM，核函数的选择是关键．核函数的选择包括两部分工作：
一是核函数类型的选择，
二是确定核函数类型后相关参数的选择．

因此如何根据具体的数据选择恰当的核函数是SVM应用领域遇到的一个重大难题，也成为科研工作者所关注的焦点，即便如此，却依然没有得到具体的理论或方法来指导核函数的选取．

## 1、经常使用的核函数
核函数的定义并不困难，根据泛函的有关理论，只要一种函数 K ( x i , x j ) 满足Mercer条件，它就对应某一变换空间的内积．对于判断哪些函数是核函数到目前为止也取得了重要的突破，得到Mercer定理和以下常用的核函数类型：

(1)线性核函数
 K ( x , x i ) = x ⋅ x i 

(2)多项式核
 K ( x , x i ) = ( ( x ⋅ x i ) + 1 ) d 

(3)径向基核（RBF）
 K ( x , x i ) = exp ( − ∥ x − x i ∥ 2 σ 2 ) 

Gauss径向基函数则是局部性强的核函数，其外推能力随着参数 σ 的增大而减弱。多项式形式的核函数具有良好的全局性质。局部性较差。

(4)傅里叶核
 K ( x , x i ) = 1 − q 2 2 ( 1 − 2 q cos ( x − x i ) + q 2 ) 

(5)样条核
 K ( x , x i ) = B 2 n + 1 ( x − x i ) 

(6)Sigmoid核函数
 K ( x , x i ) = tanh ( κ ( x , x i ) − δ ) 

采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。

## 2、核函数的选择
在选取核函数解决实际问题时，通常采用的方法有：

一是利用专家的先验知识预先选定核函数；

二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多;

三是采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想．

---

# HMM
Q:如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计(D)
A.EM算法
B.维特比算法
C.前向后向算法
D.极大似然估计

1. EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法

2. 维特比算法： 用动态规划解决HMM的预测问题，不是参数估计

维特比算法解决的是给定 一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题 Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现；

3. 前向后向：用来算概率

4. 极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数

在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。

如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据

# 特征提取算法
特征提取算法分为特征选择和特征抽取两大类
## 特征选择
常采用特征选择方法。常见的六种特征选择方法：

1. DF(Document Frequency) 文档频率
DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性

2. MI(Mutual Information) 互信息法
互信息法用于衡量特征词与文档类别直接的信息量。
如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向"低频"的特征词。
相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。

3. (Information Gain) 信息增益法
通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。

4. CHI(Chi-square) 卡方检验法
利用了统计学中的"假设检验"的基本思想：首先假设特征词与类别直接是不相关的
如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。

5. WLLR(Weighted Log Likelihood Ration)加权对数似然

6. WFO（Weighted Frequency and Odds）加权频率和可能性

## 特征抽取（降维）
 PCA

# logit 回归和 SVM 

关于 logit 回归和 SVM 不正确的是（A）

A.Logit回归目标函数是最小化后验概率
B.Logit回归可以用于预测事件发生概率的大小
C.SVM目标是结构风险最小化
D.SVM可以有效避免模型过拟合

A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。A错误 

B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确 

C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化，严格来说也是错误的。

 D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。

# L0范数与L1范数，L2范数

## L0范数

L0范数是指向量中非0的元素的个数。

如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。这太直观了，太露骨了吧，换句话说，让参数W是稀疏的。OK，看到了“稀疏”二字，大家都应该从当下风风火火的“压缩感知”和“稀疏编码”中醒悟过来，原来用的漫山遍野的“稀疏”就是通过这玩意来实现的。

但你又开始怀疑了，是这样吗？看到的papers世界中，稀疏不是都通过L1范数来实现吗？脑海里是不是到处都是||W||1影子呀！几乎是抬头不见低头见。没错，这就是这节的题目把L0和L1放在一起的原因，因为他们有着某种不寻常的关系。那我们再来看看L1范数是什么？它为什么可以实现稀疏？为什么大家都用L1范数去实现稀疏，而不是L0范数呢？

## L1范数

L1范数是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。

现在我们来分析下这个价值一个亿的问题：为什么L1范数会使权值稀疏？有人可能会这样给你回答“它是L0范数的最优凸近似”。

实际上，还存在一个更美的回答：任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。

这说是这么说，W的L1范数是绝对值，|w|在w=0处是不可微，但这还是不够直观。这里因为我们需要和L2范数进行对比分析。所以关于L1范数的直观理解，请待会看看第二节。

对了，上面还有一个问题：既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解

一是因为L0范数很难优化求解（NP难问题）
二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。
所以大家才把目光和万千宠爱转于L1范数。

一句话总结：L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。

# 为什么要稀疏？
让我们的参数稀疏有什么好处呢？这里扯两点：

1. 特征选择(Feature Selection)：

大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。

一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确yi的预测。

稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。

2. 可解释性(Interpretability)：

另一个青睐于稀疏的理由是，模型更容易解释。

例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。

假设我们这个是个回归模型： y=w1*x1+w2*x2+…+w1000*x1000+b （当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。

通过学习，如果最后学习到的w*就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个wi都非0，医生面对这1000种因素，累觉不爱。

## L2范数
除了L1范数，还有一种更受宠幸的规则化范数是L2范数: ||W||2。它也不逊于L1范数，它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。

这用的很多吧，因为它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。

至于过拟合是什么，上面也解释了，就是模型训练时候的误差很小，但在测试的时候误差很大，也就是我们的模型复杂到可以拟合到我们的所有训练样本了，但在实际预测新的样本的时候，糟糕的一塌糊涂。

通俗的讲就是应试能力很强，实际应用能力很差。擅长背诵知识，却不懂得灵活利用知识。

1. L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦。

2. 而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。为什么越小的参数说明模型越简单？

3. 我也不懂，我的理解是：限制了参数很小，实际上就限制了多项式某些分量的影响很小（看上面线性回归的模型的那个拟合的图），这样就相当于减少参数个数。其实我也不太懂，希望大家可以指点下。

一句话总结下：通过L2范数，我们可以实现了对模型空间的限制，从而在一定程度上避免了过拟合。

## L2范数的好处是什么呢？这里也扯上两点：

1）学习理论的角度：

从学习理论的角度来说，L2范数可以防止过拟合，提升模型的泛化能力。

2）优化计算的角度：

从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。哎，等等，这condition number是啥？我先google一下哈。

这里我们也故作高雅的来聊聊优化问题。优化有两大难题，
一是：局部最小值，
二是：ill-condition病态问题。

前者俺就不说了，大家都懂吧，我们要找的是全局最小值，如果局部最小值太多，那我们的优化算法就很容易陷入局部最小而不能自拔，这很明显不是观众愿意看到的剧情。

那下面我们来聊聊ill-condition。ill-condition对应的是well-condition。那他们分别代表什么？假设我们有个方程组AX=b，我们需要求解X。如果A或者b稍微的改变，会使得X的解发生很大的改变，那么这个方程组系统就是ill-condition的，反之就是well-condition的。

一句话总结：conditionnumber是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的condition number在1附近，那么它就是well-conditioned的，如果远大于1，那么它就是ill-conditioned的，如果一个系统是ill-conditioned的，它的输出结果就不要太相信了。

从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。

总结吧：L2范数不但可以防止过拟合，还可以让我们的优化求解变得稳定和快速。

# L1和L2的差别
为什么一个让绝对值最小，一个让平方最小，会有那么大的差别呢？我看到的有两种几何上直观的解析：

1. 下降速度：

  我们知道，L1和L2都是规则化的方式，我们将权值参数以L1或者L2的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。而这个最小化就像一个下坡的   过程，L1和L2的差别就在于这个“坡”不同，如下图：L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在0附近，L1的下降速度比L2的下   降速度要快。所以会非常快得降到0。不过我觉得这里解释的不太中肯，当然了也不知道是不是自己理解的问题。

2. 模型空间的限制：

  实际上，对于L1和L2规则化的代价函数来说，也就是说，我们将模型空间限制在w的一个L1-ball 中。为了便于可视化，我们考虑两维的情况，在(w1, w2)平面上可以 画出目标函数的等高线，而约束条件则成为平面上半径为C的一个 norm ball 。等高线与 norm ball 首次相交的地方就是最优解

一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。
