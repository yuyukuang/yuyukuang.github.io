---
title: 机器学习笔记（一）：NG的课程 #标题
categories: 机器学习 #分类，可多个；若单个去掉括号
tags: [深度学习, 算法] #标签，可多个；若单个去掉括号
mathjax: true #是否引用公式
kewords: #本文关键词
description: #本文描述，若为空就自动全文前150字)
copyright: true #是否需要版权申明，默认有
reward: true #是否需要打赏，默认有
toc: true #是否需要目录，默认有
password: #是否加密，为空不加密
date: 2018-1-21 16:50:02 #日期
---


# 梯度下降

1. 梯度下降的特点：梯度下降的结果一定会结束，也就是会收敛；但是收敛结果依赖于参数初始值。

2. 局部最优值：当接近局部最小值时，步子会越来越小，直到快速收敛到全局最小值。

3. 在局部最小值处，梯度是0；所以在接近局部最小值处，梯度越来越小，梯度下降每步变小。

## Batch gradient descent批梯度下降
1. 梯度下降算法的每一次迭代都要遍历整个训练集合。

2. 每次迭代之后要更新参数，这对大规模数据集不利。

3. 引入随机梯度下降或叫增量梯度下降。

4. 注意到，这里激励函数不再用sigmoid{斜率也就是梯度近似都是0，难么梯度下降学习就会很慢}，而是用RULE。

---

# Incremental gradient descent：随机梯度下降

(一)好处：

1. 为了开始修改参数，查看并利用第一个训练样本进行更新，这样依次使用第二个样本进行更新，不需要在调整之前便利所有训练数据集合。

2. 对于特定或者普通的最小二乘回归问题，梯度下降可以给出参数向量的解析表达式，这样为了求参数的值就不需要进行迭代了。

(二)坏处：

1. 不会精确的收敛到全局最小值，而是向着全局最小值附近徘徊，可能会一直徘徊。

2. 但是通常情况下，得到的结果很接近全局最小值，特别是在大规模数据训练集时，至少比批梯度下降算法要快很多。

(三) 无人驾驶:

1. 监督学习中的回归问题（梯度下降也是回归问题）。

2. 汽车尝试预测表示行驶方向的连续变量的值。


---
# 局部加权回归locally weighted regression

## 线性回归linear regression 逻辑回归logic regression 牛顿法

1. Underfitting欠拟合：数据中某些非常明显的模式没有被成功的拟合出来
2. Overfitting过拟合：算法拟合出的结果显示所给的特定数据的特质。而不是隐藏在其下的房屋价格随房屋大小变化的一般规律。
3. 过小的特征集合使得模型过于简单；过大的特征集合使得模型过于复杂
4. parametric learning algorithm参数学习算法：有固定的参数的数目进行数据拟合的算法
5. Non-parametric learning algorithm无参数学习算法：参数数目随着训练集合的大小线性增长

## 局部加权线性回归locally weighted regression：特定的无参数学习算法
  
**缺点：**

  1. 并不能完全避免过拟合和欠拟合的问题。
  2. 每次进行预测，要在一次根据整个训练集合拟合出。
  3. 若数据集大，代价高，但是可以KDtree来改善效率。

**应用：**
用在了直升机自动驾驶上。

## 为什选择最小二乘回归作为误差项的估计方法？
待更

## 似然性，中心极限定律
待更

## logic function=sigmoid function

待更

## 感知器学习算法
待更


## 牛顿学习法Newton’S Method

1. 是一种不同的用来进行模型拟合的算法，例如可以对逻辑回归模型进行拟合。这类方法通常比梯度上升算法快得多。

2. 收敛速度非常快，它的收敛速度用术语可以描述为：二次收敛。也就是，牛顿方法的每一次迭代都会使你正在逼近的解的有效数字的数目加倍。

3. 通常需要迭代十几次就可以收敛，比上述梯度上升等算法要快得多
    1. 但是，每次迭代需要重新计算一次Hession矩阵的逆（n*n, n代表特征的数量）
    2. 因此，如果要处理的问题中有大量的特征，比如说几千个，那么Hession矩阵的逆的求解要花费很大的代价。
    3. 但对于规模较小，特征数量合理的很合适。

4. 伯努利分布和高斯分布，都属于指数分布族

## 广义线性模型GLM，逻辑回归算法
待更



