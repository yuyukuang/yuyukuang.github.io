---
title: 机器学习笔记（八）：面试中常问的问题 #标题
categories: 机器学习 #分类，可多个；若单个去掉括号
tags: [深度学习, 算法,机器学习] #标签，可多个；若单个去掉括号
mathjax: true #是否引用公式
kewords: #本文关键词
description: #本文描述，若为空就自动全文前150字)
copyright: true #是否需要版权申明，默认有
reward: true #是否需要打赏，默认有
toc: true #是否需要目录，默认有
password: #是否加密，为空不加密
date: 2018-2-16 10:09:02 #日期
---


# 机器学习方面
## SVM
1. 支撑平面：和支持向量相交的平面；
   分割平面：支撑平面中间的平面。（最优分类平面）

2. SVM不是定义损失，而是定义支持向量之间的距离。

3. 正则化参数对支持向量数的影响：
   
   在任一个机器学习模型的训练过程中，被最大化或者最小化的那个函数，叫作“目标函数”（objective function）。

   目标函数可以有很多种，比如数据的（（负）对数）似然值，比如margin的大小，比如均方误差。如果目标函数是要最小化的，它就也常常被称为“损失函数”（loss function）或“费用函数”（cost function）。

   使用上面这些目标函数时，有时会发生过拟合。此时常常对模型的参数做一定的限制，使得模型偏好更简单的参数，这就叫“正则化”（regularization）。

   最常见的正则化方法，就是（软性地）限制参数的大小。设目标函数是要最小化的，所有参数组成向量w。

   如果往目标函数上加上（参数向量w的L1范数），这就是L1正则化；如果往目标函数上加上（参数向量w的L2范数的平方的一半），这就是L2正则化。

   L1正则化的优点是优化后的参数向量往往比较稀疏；L2正则化的优点是其正则化项处处可导。

   现在再看SVM的推导过程。在这个推导过程中，SVM的训练被看作一个有约束的优化问题，其目标函数是，代表了margin的大小的倒数。

   因为它是要最小化的，所以也是损失函数。而后面加上的反倒比较像正则化项，只不过它并不典型：一来它不是为了解决过拟合问题，而是为了容错；二来它并不是限制模型参数本身的大小，而是限制容错量的大小。

   不过，SVM的训练也可以从另一个角度来理解。我们不再把margin的大小作为目标函数，而是考虑分类错误所带来的代价。

   对于“+”类$（y_i =1）$的数据$x_i$，我们希望$w^Tx>0$（同样省略了bias）；
   对于“-”类$（y_i =-1）$的数据$x_i$，我们希望$w^Tx<0$：
   总之，我们希望$w^Tx_iy_i>0$。

   那么，如果实际上$w^Tx_iy_i$符号为负，或者虽然符号为正但离0不够远，具体来说是$w^Tx_iy_i<1$，我们就认为这个分类错误（或“不够正确”）带来了大小为的损失。

   于是目标函数（损失函数）就是，SVM的训练变成了这个目标函数下的无约束优化问题。

   在数据线性可分的情况下，会有许多w使得L = 0。

   为了选择一个合理的w，也需要加入正则化。加入L2正则化之后，目标函数就变成了。

   可以验证，这个目标函数跟题干中的目标函数是等价的。

   而正则化项恰巧具有物理意义；最小化正则化项，就是最大化margin。


总结一下，上述的推导过程的思路，是最大化margin（目标函数），顺便容错；而我这里给出的另一个角度，是最小化分类错误造成的损失（目标函数），顺便让margin尽可能大（正则化）。

### 西瓜书上SVM课后习题
1. 线性判别分析和线性核支持向量机在何种条件下等价。

在线性可分的情况下,LDA求出的wl与线性核支持向量机求出的ws有wl∗ws=0，即垂直，此时两者是等价的。

当初在做这个题的时候也没细想，就想当然的认为在线性可分时两者求出来的w会垂直，现在看来并不一定。 
首先，如果可以使用软间隔的线性SVM，其实线性可分这个条件是不必要的，如果是硬间隔线性SVM，那么线性可分是必要条件。这个题只说了是线性SVM，就没必要关心数据是不是可分，毕竟LDA是都可以处理的。 
第二，假如当前样本线性可分，且SVM与LDA求出的结果相互垂直。当SVM的支持向量固定时，再加入新的样本，并不会改变求出的w，但是新加入的样本会改变原类型数据的协方差和均值，从而导致LDA求出的结果发生改变。这个时候两者的w就不垂直了，但是数据依然是可分的。所以我上面说的垂直是有问题的。 
我认为这个题的答案应该就是，当线性SVM和LDA求出的w互相垂直时，两者是等价的，SVM这个时候也就比LDA多了个偏移b而已。

2. 试述高斯核SVM与RBF神经网络的联系。
![](http://p3nyp7kdl.bkt.clouddn.com/%W%}Y(H3IP`BJMOCIYMUS5G.png)
RBF网络的径向基函数与SVM都可以采用高斯核，也就分别得到了高斯核RBF网络与高斯核SVM.
* 神经网络是最小化累计误差，将参数作为惩罚项，而SVM相反，主要是最小化参数，将误差作为惩罚项。
* 在二分类问题中，如果将RBF中隐层数为样本个数，且每个样本中心就是样本参数，得出的RBF网络与核SVM基本等价，非支持向量将得到很小的W。
* 使用LIBSVM对异或问题训练一个高斯核SVM得到$\alpha$，修改第五章RBF网络的代码，固定$\beta$参数为高斯核SVM的参数，修改每个隐层神经元的中心为各个输入参数，得到结果W，w和$\alpha$各项成正比例。

上述第三条我不懂，有哪位大神看到了请给解释下，谢谢。

3. 试析SVM对噪声敏感的原因。
* SVM的目的是求出与支持向量有最大化距离的直线，以每个样本为圆心，该距离为半径做圆，可以近似认为圆内的点与该样本属于相同分类。如果出现了噪声，那么这个噪声所带来的错误分类也将最大化，所以SVM对噪声是很敏感的。

* 给定训练集，SVM最优决策边界由支持向量决定。当增加噪声时，那么该噪声有极高的可能是含噪声训练集的一个支持向量，这意味着决策边界需要改变。

## LR
1. LR的形式：h(x)=g(f(x))；其中x为原始数据；f（x）为线性/非线性回归得到的值，也叫判定边界；g（）为Sigmoid函数，最终h(x)输出范围为（0,1）
2. LR对样本分布敏感。
3. LR与NB的区别？
* LR是loss最优化求出的，NB是统计跳过LOSS最有，直接得到权重
* NB比LR多了一个条件独立假设。
     Naive Bayes是建立在条件独立假设基础之上的，设特征X含有n个特征属性（X1，X2，...Xn），那么在给定Y的情况下，X1，X2，...Xn是条件独立的。

     Logistic Regression的限制则要宽松很多，如果数据满徐条件独立假设，Logistic Regression能够取得非常好的效果；当数据不满度条件独立假设时，Logistic Regression仍然能够通过调整参数让模型最大化的符合数据的分布，从而训练得到在现有数据集下的一个最优模型。
* LR是判别模型，NB是生成模型。
     Naive Bayes是一个生成模型，在计算P(y|x)之前，先要从训练数据中计算P(x|y)和P(y)的概率，从而利用贝叶斯公式计算P(y|x)。

     Logistic Regression是一个判别模型，它通过在训练数据集上最大化判别函数P(y|x)学习得到，不需要知道P(x|y)和P(y)。
* 当数据集比较小的时候，应该选用Naive Bayes，为了能够取得很好的效果，数据的需求量为O(log n)
  当数据集比较大的时候，应该选用Logistic Regression，为了能够取得很好的效果，数据的需求量为O( n)

     Naive Bayes运用了比较严格的条件独立假设，为了计算P(y|x)，我们可以利用统计的方法统计数据集中P(x|y)和P(y)出现的次数，从而求得P(x|y)和P(y)。因而其所需的数据量要小一些，为O(log n).

     Logistic Regression在计算时，是在整个参数空间进行线性搜索的，需要的数据集就更大，为O( n)


### 什么是判别模型，什么是生成模型？
1. 判别模型是判别方法学到的模型，由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括k近邻，感知级，决策树，支持向量机等。

2. 生成模型是生成方法学到的模型，由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)。基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类，就像上面说的那样。注意了哦，这里是先求出P(X,Y)才得到P(Y|X)的，然后这个过程还得先求出P(X)。P(X)就是你的训练数据的概率分布。


### 机器学习中，LR和SVM有什么区别？
1. 两者都可以处理非线性问题；LR和SVM最初都是针对二分类问题的。

2. SVM最大化间隔平面、LR极大似然估计；SVM只能输出类别，不能给出分类概率

3. 两者loss function不同；LR的可解释性更强；SVM自带有约束的正则化

### LR为什么用sigmoid函数，这个函数有什么优点和缺点？为什么不用其他函数？（sigmoid是伯努利分布的指数族形式）

* Logistic Regression 只能用于二分类,而sigmoid对于所有的输入，得到的输出接近0或1

* Sigmoid存在的问题：梯度消失、其输出不是关于原点中心对称的（训练数据不关于原点对称时，收敛速度非常慢à输入中心对称，得到的输出中心对称时，收敛速度会非常快）、计算耗时

* Tanh激活函数存在的问题：梯 度消失、计算耗时，但是其输出是中心对称的

* ReLU：其输出不关于原点对称；反向传播时，输入神经元小于0时，会有梯度消失问题；当x=0时，该点梯度不存在（未定义）；

* ReLu失活（dead RELU）原因：权重初始化不当、初始学习率设置的非常大

* Maxout：根据设置的k值，相应的增大了神经元的参数个数

* Xavier权重初始化方法：对每个神经元的输入开根号

### SVM原问题和对偶问题关系？

* SVM对偶问题的获得方法：将原问题的目标函数L和约束条件构造拉格朗日函数，再对L中原参数和lambda、miu分别求导，并且三种导数都等于0；再将等于0的三个导数带入原目标函数中，即可获得对偶问题的目标函数

* 关系：原问题的最大值相对于对偶问题的最小值

###   KKT（Karysh-Kuhn-Tucker）条件有哪些，完整描述？

KKT条件是思考如何把约束优化转化为无约束优化à进而求约束条件的极值点
![](http://p3nyp7kdl.bkt.clouddn.com/a.png)

### 凸优化
满足三个条件：
* 在最小化（最大化）的要求下，
* 目标函数是一个凸函数（凹函数），
* 同时约束条件所形成的可行域集合是一个凸集

为什么凸优化这么重要？
局部最优即全局最优定理。

什么是凸函数？什么是凸集？什么是凸锥？
![](http://p3nyp7kdl.bkt.clouddn.com/W(K1L7CB]P89HXFFJ_B9GQU.png)

* 对于一个集合，如果集合内的任意两点构成的线段仍在集合内，则称集合为凸集。
![](http://p3nyp7kdl.bkt.clouddn.com/@U2%[GIW3(D`_Q7]LYKZ(HO.png)
* 凸锥
![](http://p3nyp7kdl.bkt.clouddn.com/b.png)
* 凸函数
![](http://p3nyp7kdl.bkt.clouddn.com/c.png)


#### 决策树过拟合哪些方法，前后剪枝

决策树对训练属性有很好的分类能力；但对位置的测试数据未必有好的分类能力，泛化能力弱，即发生过拟合。

防止过拟合的方法：剪枝（把一些相关的属性归为一个大类，减少决策树的分叉）；随机森林

#####  L1正则为什么可以把系数压缩成0，坐标回归的具体实现细节？

L1正则化可以实现稀疏（即截断），使训练得到的权重为0；

l1正则会产生稀疏解，即不相关的的特征对应的权重为0，就相当于降低了维度。但是l1的求解复杂度要高于l2,并且l1更为流行

正则化就是对loss进行惩罚（加了正则化项之后，使loss不可能为0,lambda越大惩罚越大-->lambda较小时，约束小，可能仍存在过拟合；太大时，使loss值集中于正则化的值上）

正则化使用方法：L1/L2/L1+L2

###### LR在特征较多时可以进行怎样的优化？-->L1正则有特征选择的作用

如果是离线的话，L1正则可以有稀疏解，batch大点应该也有帮助，在线的解决思路有ftrl,rds,robots,还有阿里的mlr。当然还可以用gbdt,fm,ffm做一些特性选择和组合应该也有效果。

#####  机器学习里面的聚类和分类模型有哪些？

分类：LR、SVM、KNN、决策树、RandomForest、GBDT

回归：non-Linear regression、SVR（支持向量回归-->可用线性或高斯核（RBF））、随机森林

聚类：Kmeans、层次聚类、GMM（高斯混合模型）、谱聚类
