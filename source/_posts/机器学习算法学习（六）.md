---
title: 机器学习笔记（六）：SVM #标题
categories: 机器学习 #分类，可多个；若单个去掉括号
tags: [算法, 深度学习, 机器学习] #标签，可多个；若单个去掉括号
mathjax: true #是否引用公式
kewords: #本文关键词
description: #本文描述，若为空就自动全文前150字)
copyright: true #是否需要版权申明，默认有
reward: true #是否需要打赏，默认有
toc: true #是否需要目录，默认有
password: #是否加密，为空不加密
date: 2018-2-03 18:50:02 #日期
---
![wallpaper-photo-1508367125578-6246f83905f7](http://p3nyp7kdl.bkt.clouddn.com/wallpaper-photo-1508367125578-6246f83905f7.jpg)
适用于初学SVM
<!-- more -->
# 了解SVM

支持向量机，因其英文名为support vector machine，故一般简称SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。

## 1. 分类标准的起源：Logistic回归

理解SVM，咱们必须先弄清楚一个概念：线性分类器。

给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。如果用x表示数据点，用y表示类别（y可以取1或者-1，分别代表两个不同的类），一个线性分类器的学习目标便是要在n维的数据空间中找到一个超平面（hyperplane），这个超平面的方程可以表示为（ wT中的T代表转置）
$$w^Tx+ b = 0$$
可能有读者对类别取1或-1有疑问，事实上，这个1或-1的分类标准起源于logistic回归。
$$h_\theta (x) = g(\theta^Tx) = \frac{1}{1+e^-\theta^Tx} $$

其中x是n维特征向量，函数g就是logistic函数。

$$g(z) = \frac{1}{1+e^-z}$$的图像是：

![](http://img.my.csdn.net/uploads/201304/05/1365174236_6175.png)

可以看到，上图将无穷映射到了(0,1)。

假设函数就是特征属于y=1的概率。
$$P(y=1|x; \theta) = h_\theta(x)$$
$$P(y=0|x; \theta) = 1-h_\theta(x)$$

从而，当我们要判别一个新来的特征属于哪个类时，只需求即可，若大于0.5就是y=1的类，反之属于y=0类。

此外，$h_\theta(x)$只和$\theta^Tx$有关，$\theta^Tx>0$，那么$h_\theta(x)>0.5$，而g(z)只是用来映射，真实的类别决定权还是在于$\theta^Tx$。再者，当$\theta^Tx>>0$时，$h_\theta(x)=1$，反之$h_\theta(x)=0$。如果我们只从$\theta^Tx$出发，希望模型达到的目标就是让训练数据中y=1的特征t$\theta^Tx>>0$，而是y=0的特征。Logistic回归就是要学习得到$\theta$，使得正例的特征远大于0，负例的特征远小于0，而且要在全部训练实例上达到这个目标。

接下来，尝试把logistic回归做个变形。首先，将使用的结果标签y = 0和y = 1替换为y = -1,y = 1，然后将$\theta^Tx=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n(x_0=1)$中的$\theta_0$替换为b，最后将后面的$\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n$替换为$\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n$（即$w^Tx$）。如此，则有了$\theta^Tx=w^Tx+ b$。也就是说除了y由y=0变为y=-1外，线性分类函数跟logistic回归的形式化表示$h_\theta (x) = g(\theta^Tx) =g(w^Tx+b)$没区别。

进一步，可以将假设函数中的g(z)做一个简化，将其简单映射到y=-1和y=1上。映射关系如下：
![](http://img.my.csdn.net/uploads/201304/05/1365175998_9759.png)

## 2. 线性分类的一个例子
下面举个简单的例子。如下图所示，现在有一个二维平面，平面上有两种不同的数据，分别用圈和叉表示。由于这些数据是线性可分的，所以可以用一条直线将这两类数据分开，这条直线就相当于一个超平面，超平面一边的数据点所对应的y全是-1 ，另一边所对应的y全是1。
![](http://img.blog.csdn.net/20140829134124453)

这个超平面可以用分类函数表示，当f(x) 等于0的时候，x便是位于超平面上的点，而f(x)大于0的点对应 y=1 的数据点，f(x)小于0的点对应y=-1的点，如下图所示：

![](http://img.blog.csdn.net/20140829134548371)

注：有的资料上定义特征到结果的输出函数
![](http://img.blog.csdn.net/20131120103601656)，与这里定义的$w^Tx+ b$实质是一样的。为什么？因为无论是哪个，不影响最终优化结果。下文你将看到，当我们转化到优化

![](http://img.my.csdn.net/uploads/201210/25/1351141837_7366.jpg)的时候，为了求解方便，会把yf(x)令为1，即yf(x)是y(w^x + b)，还是y(w^x - b)，对我们要优化的式子max1/||w||已无影响。
（有一朋友飞狗来自Mare_Desiderii，看了上面的定义之后，问道：请教一下SVM functional margin 为=y(wTx+b)=yf(x)中的Y是只取1和-1 吗？y的唯一作用就是确保functional margin的非负性？真是这样的么？当然不是，详情请见本文评论下第43楼）

换言之，在进行分类的时候，遇到一个新的数据点x，将x代入f(x) 中，如果f(x)小于0则将x的类别赋为-1，如果f(x)大于0则将x的类别赋为1。

接下来的问题是，如何确定这个超平面呢？从直观上而言，这个超平面应该是最适合分开两类数据的直线。而判定“最适合”的标准就是这条直线离直线两边的数据的间隔最大。所以，得寻找有着最大间隔的超平面。

## 3. 函数间隔function margin和集合间隔geometrical margin
在超平面w*x+b=0确定的情况下，|w*x+b|能够表示点x到距离超平面的远近，而通过观察w*x+b的符号与类标记y的符号是否一致可判断分类是否正确，所以，可以用(y*(w*x+b))的正负性来判定或表示分类的正确性。于此，我们便引出了函数间隔（functional margin）的概念。

定义函数间隔（用表示）为：
![](http://img.blog.csdn.net/20131107201248921)
而超平面(w，b)关于T中所有样本点(xi，yi)的函数间隔最小值（其中，x是特征，y是结果标签，i表示第i个样本），便为超平面(w, b)关于训练数据集T的函数间隔：
![](http://img.blog.csdn.net/20131111154113734)

但这样定义的函数间隔有问题，即如果成比例的改变w和b（如将它们改成2w和2b），则函数间隔的值f(x)却变成了原来的2倍（虽然此时超平面没有改变），所以只有函数间隔还远远不够。

事实上，我们可以对法向量w加些约束条件，从而引出真正定义点到超平面的距离--几何间隔（geometrical margin）的概念。

假定对于一个点 x ，令其垂直投影到超平面上的对应点为 x0 ，w 是垂直于超平面的一个向量，为样本x到超平面的距离，如下图所示
![](http://blog.pluskid.org/wp-content/uploads/2010/09/geometric_margin.png)

根据平面几何知识，有
![](http://img.blog.csdn.net/20131107201720515)

其中||w||为w的二阶范数（范数是一个类似于模的表示长度的概念），是单位向量（一个向量除以它的模称之为单位向量）。

又由于 x0 是超平面上的点，满足 f(x0)=0 ，代入超平面的方程$w^Tx+ b = 0$，可得$w^Tx_0+ b = 0$，即$w^Tx_0= -b$。

随即让此式![](http://img.blog.csdn.net/20131107201720515)的两边同时乘以$w^T$，再根据$w^Tx_0= -b$和$w^Tw= ||w||^2$，即可算出： 
![](http://img.blog.csdn.net/20131107201759093)

为了得到r的绝对值，令r乘上对应的类别 y，即可得出几何间隔（用表示）的定义：
![](http://img.blog.csdn.net/20131107201919484)

从上述函数间隔和几何间隔的定义可以看出：几何间隔就是函数间隔除以||w||，而且函数间隔y*(wx+b) = y*f(x)实际上就是|f(x)|，只是人为定义的一个间隔度量，而几何间隔|f(x)|/||w||才是直观上的点到超平面的距离。

## 4. 最大间隔分类器Maximum Margin Classifier的定义
对一个数据点进行分类，当超平面离数据点的“间隔”越大，分类的确信度（confidence）也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化这个“间隔”值。这个间隔就是下图中的Gap的一半。
![](http://img.blog.csdn.net/20140829135959290)

通过由前面的分析可知：函数间隔不适合用来最大化间隔值，因为在超平面固定以后，可以等比例地缩放w的长度和b的值，这样可以使得$f(x)=w^Tx+ b$的值任意大，亦即函数间隔可以在超平面保持不变的情况下被取得任意大。但几何间隔因为除上了||w||，使得在缩放w和b的时候几何间隔的值是不会改变的，它只随着超平面的变动而变动，因此，这是更加合适的一个间隔。换言之，这里要找的最大间隔分类超平面中的“间隔”指的是几何间隔。

于是最大间隔分类器（maximum margin classifier）的目标函数可以定义为：
$$max\hat{\gamma}$$

同时需满足一些条件，根据间隔的定义，有

![](http://img.my.csdn.net/uploads/201210/25/1351141813_4166.jpg)

其中，s.t.，即subject to的意思，它导出的是约束条件。

回顾下几何间隔的定义

![](http://img.blog.csdn.net/20131107201919484)

可知：如果令函数间隔等于1（之所以令等于1，是为了方便推导和优化，且这样做对目标函数的优化没有影响，至于为什么，请见本文评论下第42楼回复），则有$\gamma = 1 / ||w||$且![](http://img.my.csdn.net/uploads/201210/25/1351141813_4166.jpg)，从而上述目标函数转化成了

![](http://img.my.csdn.net/uploads/201210/25/1351141837_7366.jpg)

相当于在相应的约束条件![](http://img.my.csdn.net/uploads/201210/25/1351141813_4166.jpg)下，最大化这个1/||w||值，而1/||w||便是几何间隔$\hat{\gamma}$。   

如下图所示，中间的实线便是寻找到的最优超平面（Optimal Hyper Plane），其到两条虚线边界的距离相等，这个距离便是几何间隔$\hat{\gamma}$，两条虚线间隔边界之间的距离等于2$\hat{\gamma}$，而虚线间隔边界上的点则是支持向量。由于这些支持向量刚好在虚线间隔边界上，所以它们满足![](http://img.blog.csdn.net/20131111155244218)（还记得我们把 functional margin 定为 1 了吗？上节中：处于方便推导和优化的目的，我们可以令$\hat{\gamma}$=1），而对于所有不是支持向量的点，则显然有![](http://img.blog.csdn.net/20131111155205109)。

![](http://img.blog.csdn.net/20140829141714944)

OK，到此为止，算是了解到了SVM的第一层，对于那些只关心怎么用SVM的朋友便已足够，不必再更进一层深究其更深的原理。

























