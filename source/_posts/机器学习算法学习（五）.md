---
title: 机器学习笔记（五）：正则化 #标题
categories: 机器学习 #分类，可多个；若单个去掉括号
tags: [深度学习, 算法] #标签，可多个；若单个去掉括号
mathjax: true #是否引用公式
kewords: #本文关键词
description: #本文描述，若为空就自动全文前150字)
copyright: true #是否需要版权申明，默认有
reward: true #是否需要打赏，默认有
toc: true #是否需要目录，默认有
password: #是否加密，为空不加密
date: 2018-2-9 16:50:02 #日期
---

# 前言
正则化常和防止过拟合联系在一起，什么是过拟合，为什么会引起过拟合，怎么解决？
过拟合产生的原因：模型过于复杂，参数太多
什么是过拟合：模型训练时误差小，测试时误差大；也就是模型复杂到可以拟合我们所有的训练样本，但是在实际预测新样本的时候却一塌糊涂

简而言之，就是擅长背诵知识，不善于灵活应用；应试能力好，实际应用能力差。

通常过拟合由以下三种原因产生：
1. 假设过于复杂；
2. 数据存在很多噪音；
3. 数据规模太小。 

过拟合的解决方法通常有：
1. early stopping；
2.  数据集扩增；
3. 正则化；
4. Dropout。
# L0与L1-正则项（LASSO regularizer）
在机器学习里，最简单的学习算法可能是所谓的线性回归模型

![](http://p3nyp7kdl.bkt.clouddn.com/TJ{1WIHG8`9YY]($E]DG90Z.png)


我们考虑这样一种普遍的情况，即：预测目标背后的真是规律，可能只和某几个维度的特征有关；而其它维度的特征，要不然作用非常小，要不然纯粹是噪声。在这种情况下，除了这几个维度的特征对应的参数之外，其它维度的参数应该为零。若不然，则当其它维度的特征存在噪音时，模型的行为会发生预期之外的变化，导致过拟合。

于是，我们得到了避免过拟合的第一个思路：使尽可能多的参数为零。为此，最直观地我们可以引入$L_0$-范数。令：

![](http://p3nyp7kdl.bkt.clouddn.com/YJPZ6]4QI]IS1LSCF}8F(RT.png)

这意味着，我们希望绝大多数w⃗ 的分量为零。

通过引入 L0−正则项，我们实际上引入了一种「惩罚」机制，即：若要增加模型复杂度以加强模型的表达能力降低损失函数，则每次使得一个参数非零，则引入 ℓ0的惩罚系数。也就是说，如果使得一个参数非零得到的收益（损失函数上的收益）不足 ℓ0；那么增加这样的复杂度是得不偿失的。

通过引入L0−正则项，我们可以使模型稀疏化且易于解释，并且在某种意义上实现了「特征选择」。这看起来很美好，但是 L0−正则项也有绕不过去坎：

   1. 不连续
   2. 非凸
   3. 不可求导

因此，L0正则项虽好，但是求解这样的最优化问题，难以在多项式时间内找到有效解（NP-Hard 问题）。于是，我们转而考虑 L0-范数最紧的凸放松（tightest convex relaxation）：L1-范数。令：
![](http://p3nyp7kdl.bkt.clouddn.com/a.png)





# L2正则项（Ridge regularizer）
让我们回过头，考虑多项式模型，它的一般形式为：
![](http://p3nyp7kdl.bkt.clouddn.com/NHQJX9PCQA21REG`W~IQ`I1.png)

我们注意到，当多项式模型过拟合时，函数曲线倾向于靠近噪声点。

这意味着，函数曲线会在噪声点之间来回扭曲跳跃。

这也就是说，在某些局部，函数曲线的切线斜率会非常高（函数导数的绝对值非常大）。

对于多项式模型来说，函数导数的绝对值，实际上就是多项式系数的一个线性加和。

这也就是说，过拟合的多项式模型，它的参数的绝对值会非常大（至少某几个参数分量的绝对值非常大）。

因此，如果我们有办法使得这些参数的值，比较稠密均匀地集中在0附近，就能有效地避免过拟合。

于是我们引入L2−正则项，令
![](http://p3nyp7kdl.bkt.clouddn.com/WC@2B]NI1I72KSK[~50VM_S.png)

# L1−正则项与L2−正则项的区别
现在，我们考虑这样一个问题：为什么使用L1−正则项，会倾向于使得参数稀疏化；而使用L2−正则项，会倾向于使得参数稠密地接近于0？

这里引用一张来自周志华老师的著作，《机器学习》（西瓜书）里的插图，尝试解释这个问题。

![](http://omu7tit09.bkt.clouddn.com/15008845171281.png)

为了简便起见，我们只考虑模型有两个参数w1和w2的情形。

在图中，我们有三组等值线，位于同一条等值线上的w1与w2映射到相同的平方损失项、L1−范数和L2−范数。并且，对于三组等值线来说，当(w1,w2)(w1,w2)沿着等值线法线方向，向外扩张，则对应的值增大；反之，若沿着法线向内收缩，则对应的值减小。

因此，对于目标函数Obj(F)来说，实际上是要在正则项的等值线与损失函数的等值线中寻找一个交点，使得二者的和最小。

对于L1−正则项来说，因为L1−正则项是一组菱形，这些交点容易落在坐标轴上。因此，另一个参数的值在这个交点上就是0，从而实现了稀疏化。

对于 L2−正则项来说，因为 L2−正则项的等值线是一组圆形。所以，这些交点可能落在整个平面的任意位置。所以它不能实现「稀疏化」。但是，另一方面，由于 (w1,w2)(w1,w2) 落在圆上，所以它们的值会比较接近。这就是为什么 L2−正则项可以使得参数在零附近稠密而平滑。


