---
title: 算法（一） #标题
categories: 算法工程师面试 #分类，可多个；若单个去掉括号
tags: [算法面试, 深度学习面试, 机器学习面试] #标签，可多个；若单个去掉括号
mathjax: true #是否引用公式
kewords: #本文关键词
description: #本文描述，若为空就自动全文前150字)
copyright: true #是否需要版权申明，默认有
reward: true #是否需要打赏，默认有
toc: true #是否需要目录，默认有
password: #是否加密，为空不加密
date: 2018-2-9 19:09:02 #日期
---


# 第一题
1. 从1到500的500个数，第一次删除奇数位，第二次删除剩下来的奇数位，以此类推，最后剩下的唯一一位数是__

分析思路：
比如：1,2，删除奇数位，那剩下的是2，
1,2,3，删除奇数位，剩下的是2，
1,2,3,4,剩下的是4,
1,2,3,4,5,6,7,剩下的是4,
1,2,3,4,5,6,7,8和1,2,3,4,5,6,7,8,9,10,11,12,13,14,15剩下的是8，
总结规律：当1~n，$2^i<n<2^(i+1)$时候，这样删除剩下的是$2^i$。
$2^8<500<2^9$，所以剩下的就是$2^8=256$。

# 边缘算子
2. 常用边缘检测有哪些算子，有什么特性？

Roberts，Sobel，Prewitt，Laplacian，Canny

(1)Robert

优点：一种利用局部差分算子寻找边缘的算子，定位比较精确，但由于不包括平滑，所以对于噪声比较敏感。

缺点：对噪声敏感,无法抑制噪声影响。

(2)Sobel

优点：一阶微分算子，加权平均滤波，对低噪声图像有较好检测效果。

缺点：抗噪性差。

(3)Prewitt

优点：一阶微分算子，平均滤波，对低噪声图像有较好检测效果。

缺点：抗噪性差。

(4)Laplacian算子

优点：各向同性，二阶微分，精确定位边缘位置所在。

缺点：无法感知边缘强度。只适用于无噪声图象。存在噪声情况下，使用Laplacian算子检测边缘之前需要先进行低通滤波。

(5)Canny算子

是一个具有滤波，增强，检测的多阶段的优化算子。先利用高斯平滑滤波器来平滑图像以除去噪声，采用一阶偏导的有限差分来计算梯度幅值和方向，然后再进行非极大值抑制。

# 内存分配
3. C，C++程序编译的内存分配情况有哪几种，并简要解释。

C，C++程序编译的内存分配情况共有以下五种：

(1)栈区（ stack ）:由编译器自动分配释放 ，存放为运行函数而分配的局部变量、函数参数、返回数据、返回地址等。其操作方式类似于数据结构中的栈。

(2)堆区（ heap ）:一般由程序员分配释放， 若程序员不释放，程序结束时可能由 OS 回收。分配方式类似于链表。

(3)全局区（静态区）（ static ）:存放全局变量、静态数据、常量。程序结束后有系统释放

(4)文字常量区: 常量字符串就是放在这里的。 程序结束后由系统释放。

(5)程序代码区:存放函数体（类成员函数和全局函数）的二进制代码。

# cnn和fcn
4. 简述CNN和FCN的区别

卷积层是CNN区别于其它类型神经网络的本质特点

不过CNN通常也不仅仅只包含卷积层，其也会包含全连接层，全连接层的坏处就在于其会破坏图像的空间结构，因此人们便开始用卷积层来“替代”全连接层，通常采用1 × 1的卷积核，这种不包含全连接层的CNN称为全卷积网络（FCN）。

FCN最初是用于图像分割任务，之后开始在计算机视觉领域的各种问题上得到应用，事实上，Faster R-CNN中用来生成候选窗口的CNN就是一个FCN。    

# 编程题

5（编程题）给出了一个n*n的矩形，编程求从左上角到右下角的路径数（n > =2），并返回走的方法，限制只能向右或向下移动，不能回退。向右走此步骤标记为1， 向下走此步骤标记为2。
```c++
Int Solve(int b , std::vector<std::vector<int>>& method) {
} 
```
算法分析：
这是一个动态规划问题。
因为限制只能向右或向下移动，所以当走到某一步时只有两种可能：
一是从该处的上面走过来的；
二是从该处的左边走过来的。
假设我们到达某一处（i,j）, `Path[i][j]`,可以得到`Path[i][j] = Path[i - 1][j] + Path[i][j - 1]`.
上述方程的边界条件出现在最左列（`P [i] [j-1]`不存在），最上一行（`P [i-1] [j]`不存在）。这些条件可以通过初始化（预处理）来处理 - 对于所有有效的i，j，初始化`P [0] [j] = 1，P [i] [0] = 1`。注意初始值是1而不是0。

不理解上面给的b和method，这里自己写了个solution：
```c++
class Solution {
    int uniquePaths(int m, int n) {
        vector<vector<int> > path(m, vector<int> (n, 1));
        for (int i = 1; i < m; i++)
            for (int j = 1; j < n; j++)
                path[i][j] = path[i - 1][j] + path[i][j - 1];
        return path[m - 1][n - 1];
    }
};
```

6.（编程题）编程求解` a＊x + b * sin x = 0`。
```c++
Std::vector<double> Solve(double a, double b){
}
```

依然不知都写的啥，用matlab求解：
```matlab
syms a b x
solve(a＊x + b * sin x == 0) 
>>ans.x
```

7、 （探索题）你会用什么样的方法提取遥感图像中的道路呢（包括主街道、田埂、小路）。示例如下：白色明显道路是大路，深绿色田埂也属于需要提取的区域。
 ![123](http://p3nyp7kdl.bkt.clouddn.com/123.png)

（1） 对含中心线的城区主街道，采取基于模板匹配的半自动道路提取方法，可以实时准确的提取城区主干道路的中心线。以中心线上一点为中心建立对应模板窗，在沿道路前进方向寻找与之匹配的目标窗，将该目标窗中心作为下一个道路种子点，并以此生成新的模板窗，玄幻迭代即可得到一系列中心线上种子点，最后将其连成中心线。

（2） 对于田埂小路，采用基于像素匹配和基于比值直方图匹配的半自动提取算法，提取道路信息，当道路与周围环境对比度降低（周围大量的绿田），仍能保证较好结果。

8、 （随便聊聊题） 聊聊你认为深度学习中最有用的三个trick和三个创造性的idea，并说一下你的理解，为什么有用，为什么有创造性。

一．有用的三个trick

（1）loss function: 用于衡量最优的策略。一般由一个损失项和一个正则化项组成。常见的损失函数有：0-1损失函数和绝对值损失函数，log对数损失函数，平方损失函数，指数损失函数 ，Hinge损失函数。 

（2）kernel: 核函数是machine learning中最核心的部分的东西, 它有效的描述了点和点之间的关系，或者说是距离，当然这里的距离严格的说应该是广义的距离。当然核函数一般与曼哈顿距离，欧氏距离等以及空间（希尔伯特空间等）关联。

（3）activation function：激活函数不是真的要去激活什么。在神经网络中，激活函数的作用是能够给神经网络加入一些非线性因素，使得神经网络可以更好地解决较为复杂的问题。常用的三类激活函数有：

A．Sigmoid函数的输出映射在(0,1)(0,1)之间，单调连续，输出范围有限，优化稳定，可以用作输出层。

B．tanh函数比Sigmoid函数收敛速度更快

C．ReLU比起Sigmoid和tanh能够快速收敛；Sigmoid和tanh涉及了很多很expensive的操作（比如指数），ReLU可以更加简单的实现；有效缓解了梯度消失的问题；在没有无监督预训练的时候也能有较好的表现；提供了神经网络的稀疏表达能力。

二．三个创造性的idea

（1）GAN: 生成对抗神经网络。GAN已经被引入到了各种以往深度神经网络的任务中,例如从分割图像恢复原图像，给黑白图片上色，还可以做图像超分辨率,动态场景生成，图像去模糊等等。

（2）开源框架tensorflow: 不仅实现深度学习的可视化，实现了将深度学习算法移植到智能设备或手机应用中去。

（3）人脸识别，目标定位等一系列深度学习技术的成熟使人工智能走向工业智能。小米扫地机器人，物流运载机等等都可以算是深度学习的创造性成果。
