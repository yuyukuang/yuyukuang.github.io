---
title: 机器学习笔记（四）：AdaBoost算法 Boosting和Bagging等 #标题
categories: 机器学习 #分类，可多个；若单个去掉括号
tags: [深度学习, 算法] #标签，可多个；若单个去掉括号
mathjax: true #是否引用公式
kewords: #本文关键词
description: #本文描述，若为空就自动全文前150字)
copyright: true #是否需要版权申明，默认有
reward: true #是否需要打赏，默认有
toc: true #是否需要目录，默认有
password: #是否加密，为空不加密
date: 2018-1-27 16:50:02 #日期
---


# SVM
以下关于SVM说法正确的是（）
A.L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力
B.Hinge 损失函数，作用是最小化经验分类错误
C.分类间隔为1/||w||，||w||代表向量的模
D.当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习


(1). 考虑加入正则化项的原因：想象一个完美的数据集，y>1是正类，y<-1是负类，决策面y=0，加入一个y=-30的正类噪声样本，那么决策面将会变“歪”很多，分类间隔变小，泛化能力减小。A正确。

(2). 加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么“歪”了，使得分类间隔变大，提高了泛化能力。 B正确。 

(3). C错误。间隔应该是2/||w||才对，后半句应该没错，向量的模通常指的就是其二范数。 

(4). D正确。考虑软间隔的时候，C对优化问题的影响就在于把a的范围从[0，+inf]限制到了[0,C]。C越小，那么a就会越小，目标函数拉格朗日函数导数为0可以求出w=求和a_i*y_i*x_i，a变小使得w变小，因此间隔2/||w||变大

# 过拟合
在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（）
A.增加训练集量
B.减少神经网络隐藏层节点数
C.删除稀疏的特征S
D.SVM算法中使用高斯核/RBF核代替线性核

(1). 一般认为，增加隐层数可以降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化，从而增加了网络的训练时间和出现“过拟合”的倾向， 

(2). 引起过拟合的应该是太多的参数引起的。神经网络减少隐藏层节点，就是在减少参数啊，只会将训练误差变高，怎么会过拟合呢。 B错误。 

(3). 径向基(RBF)核函数/高斯核函数的说明

a. 这个核函数可以将原始空间映射到无穷维空间。

b. 对于参数 ，如果选的很大，高次特征上的权重实际上衰减得非常快，实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。

c. 不过，总的来说，通过调整参数 ，高斯核实际上具有相当高的灵活性，也是 使用最广泛的核函数 之一。 

(4). D正确。SVM高斯核函数比线性核函数模型更复杂，容易过拟合。

---
# 数据清理
数据清理中，处理缺失值的方法有两种：

1. 删除法：
       1）删除观察样本
       2）删除变量：当某个变量缺失值较多且对研究目标影响不大时，可以将整个变量整体删除
       3）使用完整原始数据分析：当数据存在较多缺失而其原始数据完整时，可以使用原始数据替代现有数据进行分析
       4）改变权重：当删除缺失数据会改变数据结构时，通过对完整数据按照不同的权重进行加权，可以降低删除缺失数据带来的偏差

2. 查补法：均值插补、回归插补、抽样填补等
成对删除与改变权重为一类
估算与查补法为一类

3. 由于调查、编码和录入误差，数据中可能存在一些无效值和缺失值，需要给予适当的处理。常用的处理方法有：估算，整例删除，变量删除和成对删除。

估算(estimation)。最简单的办法就是用某个变量的样本均值、中位数或众数代替无效值和缺失值。这种办法简单，但没有充分考虑数据中已有的信息，误差可能较大。另一种办法就是根据调查对象对其他问题的答案，通过变量之间的相关分析或逻辑推论进行估计。例如，某一产品的拥有情况可能与家庭收入有关，可以根据调查对象的家庭收入推算拥有这一产品的可能性。

整例删除(casewise deletion)是剔除含有缺失值的样本。由于很多问卷都可能存在缺失值，这种做法的结果可能导致有效样本量大大减少，无法充分利用已经收集到的数据。因此，只适合关键变量缺失，或者含有无效值或缺失值的样本比重很小的情况。

变量删除(variable deletion)。如果某一变量的无效值和缺失值很多，而且该变量对于所研究的问题不是特别重要，则可以考虑将该变量删除。这种做法减少了供分析用的变量数目，但没有改变样本量。

成对删除(pairwise deletion)是用一个特殊码(通常是9、99、999等)代表无效值和缺失值，同时保留数据集中的全部变量和样本。但是，在具体计算时只采用有完整答案的样本，因而不同的分析因涉及的变量不同，其有效样本量也会有所不同。这是一种保守的处理方法，最大限度地保留了数据集中的可用信息。

采用不同的处理方法可能对分析结果产生影响，尤其是当缺失值的出现并非随机且变量之间明显相关时。因此，在调查中应当尽量避免出现无效值和缺失值，保证数据的完整性。


# 分支定界法
1. 分支定界法（branch and bound）是一种求解 整数规划 问题的最常用算法。

2. 这种方法不但可以求解纯整数规划，还可以求解混合整数规划问题。

3. 分支定界法是计算机最擅长 的广义搜索穷举算法。

4. 分支定界法是一种搜索与迭代的方法，选择不同的分支变量和子问题进行分支。

5. 对于两个变量的整数规划问题，使用网格的方法有时更为简单。

6. 分支定界法类似决策树的决策特征，要选择那些具有强可分辨性的少量特征。


# 线性回归

关于线性回归的描述,以下正确的有:2,3,5
1. 基本假设包括随机干扰项是均值为0,方差为1的标准正态分布
2. 基本假设包括随机干扰项是均值为0的同方差正态分布
3. 在违背基本假设时,普通最小二乘法估计量不再是最佳线性无偏估计量
4. 在违背基本假设时,模型不再可以估计
5. 可以用DW检验残差是否存在序列相关性
6. 多重共线性会使得参数估计值方差减小

一元线性回归的基本假设有:
1. 随机误差项是一个期望值或平均值为0的随机变量； 
2. 对于解释变量的所有观测值，随机误差项有相同的方差； 
3. 随机误差项彼此不相关；
4. 解释变量是确定性变量，不是随机变量，与随机误差项彼此之间相互独立；
5. 解释变量之间不存在精确的（完全的）线性关系，即解释变量的样本观测值矩阵是满秩矩阵；
6. 随机误差项服从正态分布

注意：
1. 违背基本假设的计量经济学模型还是可以估计的，只是不能使用普通最小二乘法进行估计。 

2. 当存在异方差时，普通最小二乘法估计存在以下问题： 

3. 参数估计值虽然是无偏的，但不是最小方差线性无偏估计。

4. 杜宾-瓦特森（DW）检验，计量经济，统计分析中常用的一种检验序列一阶 自相关 最常用的方法。 

5. 所谓多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。影响

（1）完全共线性下参数估计量不存在

（2）近似共线性下OLS估计量非有效

多重共线性使参数估计值的方差增大，1/(1-r2)为方差膨胀因子(Variance Inflation Factor, VIF)

（3）参数估计量经济含义不合理

（4）变量的显著性检验失去意义，可能将重要的解释变量排除在模型之外

（5）模型的预测功能失效。变大的方差容易使区间预测的“区间”变大，使预测失去意义。


# SVM AdaBoost算法 Boosting和Bagging
以下说法中正确的是(BD)
A.SVM对噪声(如来自其他分布的噪声样本)鲁棒
B.在AdaBoost算法中,所有被分错的样本的权重更新比例相同
C.Boosting和Bagging都是组合多个分类器投票的方法,二者都是根据单个分类器的正确率决定其权重
D.给定n个数据点,如果其中一半用于训练,一般用于测试,则训练误差和测试误差之间的差别会随着n的增加而减少

Adaboost目的是从训练数据中学习一系列弱分类器，然后将其按一定权重累加起来得到强分类器。
刚开始每个样本对应的权重是相等的，在此样本分布下训练一个基本分类器c1.对于c1错分的样本增加其权重，对正确分类的样本降低其权重。这样使得错分的样本突出出来，并得到一个新的样本分布。同时根据分类情况赋予c1一个权重，表示其重要程度，分类正确率越高权重越大。然后在新的样本分布下对分类器进行训练，得到c2及其权重。依此类推，得到M个基本分类器及其权重。将这些弱分类器按照权重累加起来就是所期望的强分类器。（B对）

Bagging是对训练样本多次抽样训练多个分类器，然后对测试集进行投票所得到的优胜结果就是最终的分类结果。在投票时每个分类器的权重是相等的。（所以C错）

1. SVM对噪声（如来自其他分布的噪声样本）鲁棒。SVM本身对噪声具有一定的鲁棒性，但实验证明，是当噪声率低于一定水平的噪声对SVM没有太大影响，但随着噪声率的不断增加，分类器的识别率会降低。

2. 在AdaBoost算法中所有被分错的样本的权重更新比例相同。AdaBoost算法中不同的训练集是通过调整每个样本对应的权重来实现的。开始时，每个样本对应的权重是相同的，即其中n为样本个数，在此样本分布下训练出一弱分类器。对于分类错误的样本，加大其对应的权重；而对于分类正确的样本，降低其权重，这样分错的样本就被凸显出来，从而得到一个新的样本分布。在新的样本分布下，再次对样本进行训练，得到弱分类器。以此类推，将所有的弱分类器重叠加起来，得到强分类器。

3. Boost和Bagging都是组合多个分类器投票的方法，二者均是根据单个分类器正确率决定其权重。

Bagging与Boosting的区别：取样方式不同。

Bagging采用均匀取样，而Boosting根据错误率取样。

Bagging的各个预测函数没有权重，而Boosting是由权重的，Bagging的各个预测函数可以并行生成，而Boosing的哥哥预测函数只能顺序生成。

# 判别式模型与生成式模型
生成式模型(Generative Model)与判别式模型(Discrimitive Model)是分类器常遇到的概念，它们的区别在于：（对于输入x，类别标签y）

1. 生成式模型估计它们的联合概率分布P(x,y)

2. 判别式模型估计决策函数F(X)或条件概率分布P(y|x)

3. 生成式式模型可以根据贝叶斯公式得到判别式模型，但反过来不行

## 生成式模型
1. 判别式分析
2. 朴素贝叶斯Native Bayes
3. 混合高斯型Gaussians
4. K近邻KNN
5. 隐马尔科夫模型HMM
6. 贝叶斯网络
7. sigmoid belief networks
8. 马尔科夫随机场Markov random fields
9. 深度信念网络DBN
10. 隐含狄利克雷分布简称LDA(Latent Dirichlet allocation)
11. 多专家模型（the mixture of experts model）

## 判别式模型
1. 线性回归linear regression
2. 逻辑回归logic regression
3. 神经网络NN
4. 支持向量机SVM
5. 高斯过程Gaussian process
6. 条件随机场CRF
7. CART(Classification and regression tree)
8. Boosting

# 线性分类器
线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。

1. 感知器准则函数：代价函数J=-(W*X+w0).分类的准则是最小化代价函数。感知器是神经网络（NN）的基础，网上有很多介绍。

2. SVM：支持向量机也是很经典的算法，优化目标是最大化间隔（margin），又称最大间隔分类器，是一种典型的线性分类器。（使用核函数可解决非线性问题）

3. Fisher准则：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。

4. 贝叶斯分类器：一种基于统计方法的分类器，要求先了解样本的分布特点（高斯、指数等），所以使用起来限制很多。在满足一些特定条件下，其优化目标与线性分类器有相同结构（同方差高斯分布等），其余条件下不是线性分类。

## 分类问题
在分类问题中,我们经常会遇到正负样本数据量不等的情况,比如正样本为10w条数据,负样本只有1w条数据,以下最合适的处理方法是()
A.将负样本重复10次,生成10w样本量,打乱顺序参与分类
B.直接进行分类,可以最大限度利用数据
C.从10w正样本中随机抽取1w参与分类
D.将负样本每个权重设置为10,正样本权重为1,参与训练过程

解决这类问题主要分：

1. 重采样。A可视作重采样的变形。改变数据分布消除不平衡，可能导致过拟合。

2. 欠采样。C的方案 提高少数类的分类性能，可能丢失多数类的重要信息。

3. 权值调整。

如果1：10算是均匀的话，可以将多数类分割成为1000份。然后将每一份跟少数类的样本组合进行训练得到分类器。而后将这1000个分类器用assemble的方法组合位一个分类器。A选项可以看作此方式，因而相对比较合理。

另：如果目标是 预测的分布 跟训练的分布一致，那就加大对分布不一致的惩罚系数。



D方案也是其中一种方式。

## 类域界面方程法
类域界面方程法中，不能求线性不可分情况下分类问题近似或精确解的方法是？
A.伪逆法
B.感知器算法
C.基于二次准则的H-K算法
D.势函数法

伪逆法：径向基（RBF）神经网络的训练算法，径向基解决的就是线性不可分的情况。

感知器算法：线性分类模型。线性不可分时，感知器算法不收敛。

H-K算法：在最小均方误差准则下求得权矢量，二次准则解决非线性问题。

基于二次准则函数的H-K算法较之于感知器算法的优点是:可以判别问题是否线性可分,其解的适应性更好.

势函数法：势函数非线性。

# 主分量（主成分）分析PCA
已知一组数据的协方差矩阵P,下面关于主分量说法错误的是(C)
A.主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小
B.在经主分量分解后,协方差矩阵成为对角矩阵
C.主分量分析就是K-L变换
D.主分量是通过求协方差矩阵的特征值得到

K-L变换与PCA变换是不同的概念

PCA的变换矩阵是协方差矩阵

K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）

当K-L变换矩阵为协方差矩阵时，等同于PCA。
